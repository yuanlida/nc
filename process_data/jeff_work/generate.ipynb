{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "752\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "digits = random.randint(0,999)\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def replacer(s, newstring, index, nofail=False):\n",
    "    # raise an error if index is outside of the string\n",
    "    if not nofail and index not in range(len(s)):\n",
    "        raise ValueError(\"index outside given string\")\n",
    "\n",
    "    # if not erroring, but the index is still not in the correct range..\n",
    "    if index < 0:  # add it to the beginning\n",
    "        return newstring + s\n",
    "    if index > len(s):  # add it to the end\n",
    "        return s + newstring\n",
    "\n",
    "    # insert the new string between \"slices\" of the original\n",
    "    return s[:index] + newstring + s[index + 1:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 22994.84it/s]\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "ok\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## Generate Phone numbers in multiple formats that follow US rule\n",
    "## 1 - NXX - NXX - XXXX     (N = (2,9)   X = (0,9)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from random import randrange\n",
    "import random\n",
    "num1 = \"( 209 ) 647 - 4545\"\n",
    "num2 = \"1 - 800 - 422 - 4453\"\n",
    "num3 = \"p  1 - 713 - 520 - 1462\"\n",
    "num4= \"p  707 - 654 - 1173\"\n",
    "num5 = \"M: 07973 734 297\"\n",
    "num6 = \"M: 0435148612\" ## IMPT\n",
    "num7 = \"|t. +618 9387 3156\"\n",
    "num8 = \"Phone - 610-871-3935 Ext. 34 \"\n",
    "num9 = \"Cell - 610-442-4738\"\n",
    "num10 =\"Telephone 020 8319 9800 Ext 2215 863\"\n",
    "num11 = \"941 312 1214\"\n",
    "num12 = \"Office 510.651.6500 | cell 718.607.5656\"\n",
    "## OFFICE: 407.545.3030 ext. 221 CELL: 407.221.7000 | label is  org\n",
    "## t.  613.606.3608\n",
    "## M: 0435148612\n",
    "## Office: 501.553.9321\n",
    "## office: 205.879.3282 x5548\n",
    "#~~~~~~~~~M12~~~~~~~~~~~~~~~~\n",
    "## OFFICE: 407.545.3030 ext. 221 CELL: 407.221.7000 | label is  org\n",
    "# Office: 501.553.9321 Mobile: 501.909.9243\n",
    "# Office: 501.553.9321 \n",
    "# + 34 619730167 \n",
    "# Tel: 4087 0408 \n",
    "# Mobile: 610-564-1243 \n",
    "\n",
    "hp_type = ['Mobile: ', 'OFFICE:', 'Office:', 'CELL:']\n",
    "tel_type = ['M: ', 'Tel: ', 'O: ', 'Office: ', 'Cell: ', 'Phone: ', 'Mobile: ', 'Telephone: ', 't. ', '']\n",
    "ext_type = ['x', 'Ext. ', 'ext. ']\n",
    "caps_type = ['OFFICE:', 'CELL:', 'HOME:']\n",
    "mark = ['.', ' ', '-', '']\n",
    "# with open('/Users/macos/thunder/ner-tf/cls_data/phone.txt', 'w') as w:\n",
    "\n",
    "area_code = \"240\"\n",
    "# print(\"Your new phone number is ({}) {}{}{}-{}{}{}{}\".format(area_code, *[randrange(10) for i in range(7)]))\n",
    "\n",
    "lines_seen = set()\n",
    "# with open('/Users/macos/thunder/ner-tf/cls_data/phone_all.txt', 'w') as w:\n",
    "with open('/Users/macos/thunder/ner-tf/cls_data/extra/phone_all5.txt', 'w') as w:    \n",
    "    for i in tqdm(range(2000)):\n",
    "        # phone1 = ( \"({}{}{}) {}{}{} - {}{}{}{}\".format(randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(7)]))\n",
    "        # phone2 = ( \"1 - {}{}{} - {}{}{} - {}{}{}{}\".format(randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(7)]))\n",
    "        # phone3 = ( \"p {}{}{} . {}{}{} . {}{}{}{}\".format(randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(7)]))\n",
    "        # phone4 = ( \"p {}{}{} - {}{}{} - {}{}{}{}\".format(randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(7)]))\n",
    "        # phone5 = ( \"p 1 - {}{}{} - {}{}{} - {}{}{}{}\".format(randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(7)]))\n",
    "        # phone1 = ( \"{}{}{}{} {}{}{} {}{}{}{}\".format(random.choice(hp_type), randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(7)]))\n",
    "        # phone2 = ( \"{}{}{}{}{}{}{}{}{}{}{}\".format(random.choice(hp_type), randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(7)]))\n",
    "        # phone3 = ( \"{}{}{}{}.{}{}{}.{}{}{}{} | {}{}{}{}.{}{}{}.{}{}{}{}\".format\n",
    "        #            (random.choice(hp_type), randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(6)],\n",
    "        #             random.choice(hp_type), randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(6)]))\n",
    "        # phone4 = ( \"{}{}{}{} {}{}{} {}{}{}{} Ext {}\".format(random.choice(hp_type), randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(6)], random.randint(0,99999)))\n",
    "        # phone5 = ( \"{}{}{}{}{} {}{}{} {}{}{}{}\".format(random.choice(tel_type), random.choice(ext_type), randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(5)], random.randint(0,99)))\n",
    "        phone1 = ( \"Text // {}{}{}-{}{}{}-{}{}{}{}\".format(*[randrange(10) for i in range(10)], random.randint(0,999)))\n",
    "        phone2 = ( \"t. {}{}{}.{}{}{}.{}{}{}{}\".format(*[randrange(10) for i in range(10)]))\n",
    "\n",
    "        # phone2 = ( \"{}{}{}{}{} {}{}{}{}\".format(random.choice(tel_type), randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(6)]))\n",
    "        # phone3 = ( \"{}{}{}{}.{}{}{}.{}{}{}{}\".format\n",
    "        #            (random.choice(hp_type), randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(6)]))\n",
    "        # phone4 = ( \"{}{}{}{}.{}{}{}.{}{}{}{} {}{}{}{}.{}{}{}.{}{}{}{}\".format(random.choice(tel_type), randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(6)],\n",
    "        #                                                                               random.choice(tel_type), randrange(2,10), randrange(10), randrange(10), randrange(2,10), *[randrange(10) for i in range(6)]))\n",
    "        # \n",
    "        # phone5 = ( \"+ {}{} {}\".format(randrange(0,10), randrange(0,10), random.randint(10000000,999999999)))\n",
    "# Text // 202-630-4865 \n",
    "# t.  613.606.3608\n",
    "        \n",
    "# Office: 501.553.9321 Mobile: 501.909.9243\n",
    "# Office: 501.553.9321 \n",
    "# + 34 619730167 \n",
    " #       OFFICE: 407.545.3030 ext. 221\n",
    "# Tel: 4087 0408 \n",
    "# Mobile: 610-564-1243 \n",
    "        if phone1 not in lines_seen:\n",
    "            lines_seen.add(phone1)\n",
    "            w.writelines(phone1)\n",
    "            w.writelines('\\n')\n",
    "            \n",
    "        if phone2 not in lines_seen:\n",
    "            lines_seen.add(phone2)\n",
    "            w.writelines(phone2)\n",
    "            w.writelines('\\n')\n",
    "        #     \n",
    "        # if phone3 not in lines_seen:\n",
    "        #     lines_seen.add(phone3)\n",
    "        #     w.writelines(phone3)\n",
    "        #     w.writelines('\\n')\n",
    "        #     \n",
    "        # if phone4 not in lines_seen:\n",
    "        #     lines_seen.add(phone4)\n",
    "        #     w.writelines(phone4)\n",
    "        #     w.writelines('\\n')\n",
    "        # #     \n",
    "        # if phone5 not in lines_seen:\n",
    "        #     lines_seen.add(phone5)\n",
    "        #     w.writelines(phone5)\n",
    "        #     w.writelines('\\n')\n",
    "            \n",
    "print('ok')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "100%|██████████| 11401/11401 [00:00<00:00, 1053589.35it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 29355.02it/s]\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "finish building list!\n",
      "ok\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## Data Augmentation\n",
    "## Generate Location based on US elements\n",
    "\n",
    "import random\n",
    "from random import randrange\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "city_file = '/Users/macos/thunder/ner-tf/jeff_work/place_list/city.txt'\n",
    "state_file = '/Users/macos/thunder/ner-tf/jeff_work/place_list/state.txt'\n",
    "street_file = '/Users/macos/thunder/ner-tf/jeff_work/place_list/street.txt'\n",
    "street_only = '/Users/macos/thunder/ner-tf/jeff_work/place_list/str_only.txt'\n",
    "countries_file ='/Users/macos/thunder/ner-tf/cls_data/extra/world.txt'\n",
    "fake_add = '/Users/macos/thunder/ner-tf/cls_data/extra/unit_lobby.txt'\n",
    "# 7 W . Bridge Rd . , Oswego , New York , 13126 \n",
    "# 445 St . Paul St . , Rochester , New York , 14605 \n",
    "# 411 Cornelia St . , Plattsburgh , New York , 12901 \n",
    "# 223 Reindollar Ave . , Marina , California , 93933\n",
    "# 1200 Roseville Pkwy . , Roseville , California , 95678\n",
    "# 10450 Friar ' s Road - Suite L , San Diego , California , 92120\n",
    "## Qa\n",
    "\n",
    "streets = []\n",
    "s_type = ['St .', 'Rd .', 'Av .', 'Road', 'Street', 'Avenue', 'Lane', 'Dr .', 'Drive']\n",
    "cities = []\n",
    "states = []\n",
    "s_only = []\n",
    "countries = []\n",
    "punc = [',', '']\n",
    "# st_num = ['', (str(random.randint(0,9999)) + ' ')]\n",
    "with open (city_file, 'r') as r, open(state_file, 'r') as s, open(countries_file, 'r') as v,\\\n",
    "        open(street_only, 'r') as u, open(street_file, 'r') as t:\n",
    "    for str in tqdm(t.readlines()):\n",
    "        streets.append(str.title().rstrip())\n",
    "    for ct in r.readlines():\n",
    "        cities.append(ct.rstrip())\n",
    "    for sta in s.readlines():\n",
    "        states.append(sta.rstrip())\n",
    "    for only in u.readlines():\n",
    "        s_only.append(only.rstrip())\n",
    "    for count in v.readlines():\n",
    "        countries.append(count.rstrip())\n",
    "    print('finish building list!')\n",
    "\n",
    "lines_seen = set()\n",
    "with open(fake_add, 'w') as w:\n",
    "    for i in tqdm(range(2000)):\n",
    "        \n",
    "        # 26 West 9th Street Suite 4D, New York, NY 10011\n",
    "        # Family First   5509 W Gray Street  Suite 100  Tampa,  FL   33609   United States\n",
    "        # 1 Infinite Loop, MS 96-DM, Cupertino, CA 95014.\n",
    "        # 2054 University Ave,Berkeley,CA'\n",
    "        # 11 Walling Road'\n",
    "        # Steinwiesstrasse 52 CH-8032 Zürich\n",
    "        # 'Unit 9 Wildmoor Mill | Mill Lane | Wildmoor | Worcestershire | B61 0BX\n",
    "        # st_num = ['', \"{} \".format(random.randint(0,9999))]\n",
    "        #Level #03-20\n",
    "        #Galaxis West Lobby\n",
    "        # Suite A-220 \n",
    "        \n",
    "        full = ( \"Level #{}{}-{}{}\".format(*[randrange(10) for i in range(4)]))\n",
    "        \n",
    "        full1 = ( \"{} Lobby\".format(random.choice(streets)))\n",
    "        \n",
    "        full2 = ( \"Suite {}-{}\".format(random.choice(string.ascii_uppercase), random.randint(0,999)))\n",
    "        # full1 = (\"{} {}  {},  {}  {}{}{}{}{}\".format(random.randint(0,999),\n",
    "        #                             random.choice(streets), random.choice(cities),\n",
    "        #                             random.choice(states), *[randrange(10) for i in range(5)]))\n",
    "        # full2 = (\"{} {} {}\".format(random.randint(0,999), # full = (\"{} {} , {} , {} {}{}{}{}{}\".format(random.randint(0,999),\n",
    "        #                             random.choice(streets), random.choice(cities),\n",
    "        #                             random.choice(states), *[randrange(10) for i in range(5)]))\n",
    "        # \n",
    "      \n",
    "        if full not in lines_seen:\n",
    "            lines_seen.add(full)\n",
    "            w.writelines(full)\n",
    "            w.writelines('\\n')\n",
    "            \n",
    "        if full1 not in lines_seen:\n",
    "            lines_seen.add(full1)\n",
    "            w.writelines(full1)\n",
    "            w.writelines('\\n')\n",
    "\n",
    "        if full2 not in lines_seen:\n",
    "            lines_seen.add(full2)\n",
    "            w.writelines(full2)\n",
    "            w.writelines('\\n')\n",
    "\n",
    "        # if full3 not in lines_seen:\n",
    "        #     lines_seen.add(full3)\n",
    "        #     w.writelines(full3)\n",
    "        #     w.writelines('\\n')\n",
    "print('ok')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b00b6872a3d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# print(doc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'and'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'and' is not in list"
     ],
     "ename": "ValueError",
     "evalue": "'and' is not in list",
     "output_type": "error"
    }
   ],
   "source": [
    "## Segment job variations to generate multiple permutations\n",
    "# import inflect\n",
    "import inflect\n",
    "inflect = inflect.engine()\n",
    "line = 'Counter and Rental Clerk'\n",
    "line1= \"Farmers, Ranchers, and Other Agricultural Manager\"\n",
    "# line2 = \"Securities, Commodities, and Financial Services Sales Agent\"\n",
    "line2 = \"Legislators\"\n",
    "# line2 = \"Agricultural and Food Science Technician\"\n",
    "doc = line2.split()\n",
    "# print(doc)\n",
    "n = doc.index('and')\n",
    "print(n)\n",
    "tit = ''\n",
    "for i,word in enumerate(doc):\n",
    "    if word.endswith(','): #or doc[i+1]!='and':\n",
    "        tit = word[:-1] + ' ' + doc[-1]\n",
    "        print('1' + tit)\n",
    "        tit = ''\n",
    "    elif word==('and') and (doc[i-1].endswith(',')==False):\n",
    "        tit+=doc[-1]\n",
    "        print('2' + tit)\n",
    "        tit = ''\n",
    "        continue\n",
    "    elif word==('and') and (doc[i-1].endswith(',')==True):\n",
    "        tit+=doc[-1]\n",
    "        print('2' + tit)\n",
    "        tit = ''\n",
    "        continue\n",
    "    elif word == doc[-1]:\n",
    "        tit+= word\n",
    "        print('3' + tit)\n",
    "        continue\n",
    "    else:\n",
    "        tit+= word + ' '\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "100%|██████████| 164433/164433 [00:00<00:00, 1435420.00it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 133819.48it/s]\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "finish building list!\n",
      "finish building list!\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## svm \n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "fn_file = '/Users/macos/Desktop/Edison-ai/data/ner/names/first_names.all.txt'\n",
    "ln_file = '/Users/macos/Desktop/Edison-ai/data/ner/names/last_names.all.txt'\n",
    "name_file = '/Users/macos/thunder/ner-tf/cls_data/extra/fake_name3.txt'\n",
    "\n",
    "\n",
    "first_name = []\n",
    "last_name = []\n",
    "upper_alphabet = string.ascii_uppercase\n",
    "formater = [\"({}) {}\", \"{} {}\", \"{} ({})\"]\n",
    "number  = [0, 1, 2]\n",
    "with open (fn_file, 'r') as r, open(ln_file, 'r') as s:\n",
    "    for fn in tqdm(r.readlines()):\n",
    "        first_name.append(fn.rstrip().title())\n",
    "    for ln in s.readlines():\n",
    "        last_name.append(ln.rstrip().title())\n",
    "        \n",
    "print('finish building list!')\n",
    "##FLAMINIANO Ronald Jonn Villamil\n",
    "# Evan de Barra AKA Karl\n",
    "# Rigo Zamarripa | Neighbor Ambassador | PROMETHEUS REAL ESTATE GROUP, INC. | Est. 1965\n",
    "# name | title | org | \n",
    "lines_seen = set()\n",
    "with open(name_file, 'w') as w:\n",
    "    for i in tqdm(range(3000)):\n",
    "        n = random.choice(number)\n",
    "        if n==0:\n",
    "            full = (\"{} {} {}\".format(random.choice(first_name).upper(), random.choice(first_name).upper(), random.choice(last_name).upper()))\n",
    "        elif n ==1:\n",
    "            full = (\"{} {}\".format(random.choice(first_name).upper(), random.choice(last_name).upper()))\n",
    "            \n",
    "        else:\n",
    "            full = (\"{}\".format(random.choice(first_name).upper()))\n",
    "\n",
    "        if full not in lines_seen:\n",
    "            lines_seen.add(full)\n",
    "            w.writelines(full)\n",
    "            w.writelines('\\n')\n",
    "        #     \n",
    "        # if full1 not in lines_seen:\n",
    "        #     lines_seen.add(full1)\n",
    "        #     w.writelines(full1)\n",
    "        #     w.writelines('\\n')\n",
    "        #     \n",
    "        # if full2 not in lines_seen:\n",
    "        #     lines_seen.add(full2)\n",
    "        #     w.writelines(full2)\n",
    "        #     w.writelines('\\n')\n",
    "\n",
    "print('finish building list!')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## fake names\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "fn_file = '/Users/macos/Desktop/Edison-ai/data/ner/names/first_names.all.txt'\n",
    "ln_file = '/Users/macos/Desktop/Edison-ai/data/ner/names/last_names.all.txt'\n",
    "name_file = '/Users/macos/thunder/ner-tf/cls_data/extra/fake_name3.txt'\n",
    "\n",
    "\n",
    "first_name = []\n",
    "last_name = []\n",
    "upper_alphabet = string.ascii_uppercase\n",
    "formater = [\"({}) {}\", \"{} {}\", \"{} ({})\"]\n",
    "number  = [0, 1, 2]\n",
    "with open (fn_file, 'r') as r, open(ln_file, 'r') as s:\n",
    "    for fn in tqdm(r.readlines()):\n",
    "        first_name.append(fn.rstrip().title())\n",
    "    for ln in s.readlines():\n",
    "        last_name.append(ln.rstrip().title())\n",
    "        \n",
    "print('finish building list!')\n",
    "##FLAMINIANO Ronald Jonn Villamil\n",
    "# Evan de Barra AKA Karl\n",
    "# Rigo Zamarripa | Neighbor Ambassador | PROMETHEUS REAL ESTATE GROUP, INC. | Est. 1965\n",
    "# name | title | org | \n",
    "lines_seen = set()\n",
    "with open(name_file, 'w') as w:\n",
    "    for i in tqdm(range(3000)):\n",
    "        n = random.choice(number)\n",
    "        if n==0:\n",
    "            full = (\"{} {} {}\".format(random.choice(first_name).upper(), random.choice(first_name).upper(), random.choice(last_name).upper()))\n",
    "        elif n ==1:\n",
    "            full = (\"{} {}\".format(random.choice(first_name).upper(), random.choice(last_name).upper()))\n",
    "            \n",
    "        else:\n",
    "            full = (\"{}\".format(random.choice(first_name).upper()))\n",
    "\n",
    "        if full not in lines_seen:\n",
    "            lines_seen.add(full)\n",
    "            w.writelines(full)\n",
    "            w.writelines('\\n')\n",
    "        #     \n",
    "        # if full1 not in lines_seen:\n",
    "        #     lines_seen.add(full1)\n",
    "        #     w.writelines(full1)\n",
    "        #     w.writelines('\\n')\n",
    "        #     \n",
    "        # if full2 not in lines_seen:\n",
    "        #     lines_seen.add(full2)\n",
    "        #     w.writelines(full2)\n",
    "        #     w.writelines('\\n')\n",
    "\n",
    "print('finish building list!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('start!')\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Remove names from ORG\n",
    "name_list = ['bob', 'bob lim', 'jeff']\n",
    "lines = \"\"\"CLAWI - Mundo Gaturro\n",
    "CityOdds\n",
    "BettingVille\n",
    "Ototespit . com\n",
    "iLogistix\n",
    "MotorExchange\n",
    "Market Research\n",
    "Fingerworks\n",
    "Ladenburg Thalmann\n",
    "Encryptosoft\n",
    "PolySpot Enterprise Search\n",
    "bob works\n",
    "bob\n",
    "TrenMedia\n",
    "jeffur\n",
    "JadeTrack\"\"\"\n",
    "# name_file = '/Users/macos/Desktop/nc/data/train/name.txt'\n",
    "name_file = '/Users/macos/thunder/ner-tf/cls_data/clean.txt'\n",
    "# org_file  = '/Users/macos/Desktop/nc/data/train/org.txt'\n",
    "org_file = '/Users/macos/thunder/ner-tf/jeff_work/process_data/hold.txt'\n",
    "new_org = '/Users/macos/thunder/ner-tf/jeff_work/new_tel.txt'\n",
    "\n",
    "body = []\n",
    "name_list = []\n",
    "with open(name_file, 'r') as r, open(org_file, 'r') as s, open(new_org, 'w') as w:\n",
    "    name_body = r.readlines()\n",
    "    for name in tqdm(name_body):\n",
    "        name_list.append(name)\n",
    "        \n",
    "    org_body = s.readlines()\n",
    "    for org in tqdm(org_body):\n",
    "        body.append(org)\n",
    "    # body = org_body.split('\\n')\n",
    "    print('time to check!')\n",
    "# body = lines.split('\\n')\n",
    "# [i for i in name_list if i in body]\n",
    "# print(i)\n",
    "    for i in tqdm(name_list):\n",
    "        if i in body:\n",
    "            body.remove(i)\n",
    "            # print(i)\n",
    "        # else:\n",
    "        #     w.writelines(i)\n",
    "        #     w.writelines('\\n')\n",
    "    for ele in body:\n",
    "        w.writelines(ele)\n",
    "        # w.writelines('\\n')\n",
    "print('ok')\n",
    "# for line in lines:\n",
    "#     if name_list in :\n",
    "#         print(line)\n",
    "#     else:\n",
    "#         print('nope!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "100%|██████████| 9659/9659 [21:26<00:00,  7.51it/s] \n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "start!\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('start!')\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import random\n",
    "os.environ[\"MODEL_DIR\"] = '../model'\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "# name_file = '/Users/macos/thunder/ner-tf/cls_data/extra/fake_name.txt'\n",
    "name_tel = '/Users/macos/thunder/ner-tf/jeff_work/process_data/hold.txt'\n",
    "swap_tel = '/Users/macos/thunder/ner-tf/cls_data/extra/swap_tel.txt'\n",
    "\n",
    "option = [1,2,3,4,5]\n",
    "with open(name_tel, 'r') as r, open(swap_tel, 'w') as w:\n",
    "    \n",
    "    aug1= naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path='/Users/macos/Desktop/Edison-ai/models/GoogleNews-vectors-negative300.bin',\n",
    "    action=\"insert\", aug_max=2)\n",
    "    ##\n",
    "    aug2 = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path='/Users/macos/Desktop/Edison-ai/models/GoogleNews-vectors-negative300.bin',\n",
    "    action=\"insert\", aug_max=2)\n",
    "    aug3 = nac.RandomCharAug(action=\"insert\")\n",
    "    aug4 = nac.KeyboardAug()\n",
    "    aug5 = nac.RandomCharAug(action=\"delete\")\n",
    "    \n",
    "    body = r.readlines()\n",
    "    for text in tqdm(body):\n",
    "        n = random.choice(option)\n",
    "        if n == 1:\n",
    "            augmented_text = aug1.augment(text)\n",
    "        if n == 2:\n",
    "            augmented_text = aug2.augment(text)\n",
    "        if n == 3:\n",
    "            augmented_text = aug3.augment(text)\n",
    "        if n == 4:\n",
    "            augmented_text = aug4.augment(text)\n",
    "        else:\n",
    "            augmented_text = aug5.augment(text)\n",
    "        w.writelines(augmented_text)#%%\n",
    "        w.writelines('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "start!\n",
      "ok\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "100%|██████████| 10657/10657 [00:07<00:00, 1413.92it/s]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## aug TIT\n",
    "print('start!')\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import random\n",
    "os.environ[\"MODEL_DIR\"] = '../model'\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "# name_file = '/Users/macos/thunder/ner-tf/cls_data/extra/fake_name.txt'\n",
    "# tit_file = '/Users/macos/Desktop/Edison-ai/data/sig-extract/tord.txt'\n",
    "tit_file = '/Users/macos/thunder/ner-tf/jeff_work/original/tit.txt'\n",
    "swap_tit = '/Users/macos/thunder/ner-tf/jeff_work/original/lean_tit.txt'\n",
    "\n",
    "option = [3,4,5]\n",
    "sim = [1,2]\n",
    "with open(tit_file, 'r') as r, open(swap_tit, 'w') as w:\n",
    "    aug0 = naw.SynonymAug(aug_src='wordnet', aug_min = 1, aug_max=1)\n",
    "    # aug1 = naw.WordEmbsAug(\n",
    "    # model_type='word2vec', model_path='/Users/macos/Desktop/Edison-ai/models/GoogleNews-vectors-negative300.bin',\n",
    "    # action=\"insert\", aug_min = 1)\n",
    "    ##\n",
    "    # aug2 = naw.WordEmbsAug(\n",
    "    # model_type='word2vec', model_path='/Users/macos/Desktop/Edison-ai/models/GoogleNews-vectors-negative300.bin',\n",
    "    # action=\"substitute\", aug_min = 1)\n",
    "    # aug3 = nac.RandomCharAug(action=\"insert\", aug_char_max=2, aug_word_max=2, aug_word_min=1)\n",
    "    # aug4 = nac.KeyboardAug(aug_char_max=1, aug_word_max=1, aug_word_min=1)\n",
    "    # aug5 = nac.RandomCharAug(action=\"delete\",  aug_char_max=2, aug_word_max=2, aug_word_min=1)\n",
    "    \n",
    "\n",
    "    body = r.readlines()\n",
    "    # for i in tqdm(range(5)):\n",
    "    for no, text in enumerate(tqdm(body)):\n",
    "        try:\n",
    "            # if text.startswith('###############') == False and (text.startswith('#sig#') == False):\n",
    "            # n = random.choice(option)\n",
    "            # if n == 3:\n",
    "            #     augmented_text = aug3.augment(text)\n",
    "            # elif n == 4:\n",
    "            # augmented_text = aug4.augment(text)\n",
    "            # else:\n",
    "            #     augmented_text = aug5.augment(text)\n",
    "                    \n",
    "            sub_text = aug0.augment(text)\n",
    "        except:\n",
    "            print('error is in line:', no)\n",
    "        w.writelines(text)\n",
    "        w.writelines('\\n')\n",
    "        w.writelines(sub_text)\n",
    "        w.writelines('\\n')\n",
    "        # w.writelines(augmented_text)\n",
    "        # w.writelines('\\n')\n",
    "    print('ok')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "start!\n",
      "ok!\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "100%|██████████| 5555/5555 [06:56<00:00, 13.35it/s]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## augLOC\n",
    "print('start!')\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import random\n",
    "os.environ[\"MODEL_DIR\"] = '../model'\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "\n",
    "# name_file = '/Users/macos/thunder/ner-tf/cls_data/extra/fake_name.txt'\n",
    "tit_file = '/Users/macos/thunder/ner-tf/cls_data/clean.txt'\n",
    "swap_tit = '/Users/macos/thunder/ner-tf/cls_data/extra/aug_unit_lobby.txt'\n",
    "\n",
    "option = [2,3,4,5,6]\n",
    "sim = [1,2]\n",
    "with open(tit_file, 'r') as r, open(swap_tit, 'w') as w:\n",
    "    \n",
    "    # aug0 = naw.SynonymAug(aug_src='wordnet', aug_min = 1)\n",
    "    # aug8 = naw.WordEmbsAug(\n",
    "    # model_type='word2vec', model_path='/Users/macos/Desktop/Edison-ai/models/GoogleNews-vectors-negative300.bin',\n",
    "    # action=\"insert\", aug_min = 1)\n",
    "    # ##\n",
    "    # aug2 = naw.WordEmbsAug(\n",
    "    aug2 = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path='/Users/macos/Desktop/Edison-ai/models/GoogleNews-vectors-negative300.bin',\n",
    "    action=\"substitute\", aug_max = 1)\n",
    "    aug3 = nac.RandomCharAug(action=\"insert\")\n",
    "    aug4 = nac.KeyboardAug()\n",
    "    aug5 = nac.RandomCharAug(action=\"delete\")\n",
    "    # aug6 = naw.RandomWordAug(action= \"insert\")\n",
    "    # aug7 = naw.RandomWordAug(action= \"delete\")\n",
    "    aug6 = naw.RandomWordAug(action= \"swap\")\n",
    "    \n",
    "    body = r.readlines()\n",
    "    for no, text in enumerate(tqdm(body)):\n",
    "        try:\n",
    "            n = random.choice(option)\n",
    "            if n == 3:\n",
    "                augmented_text = aug3.augment(text)\n",
    "            elif n == 2:\n",
    "                augmented_text = aug2.augment(text)\n",
    "            elif n == 4:\n",
    "                augmented_text = aug4.augment(text)\n",
    "            elif n == 5:\n",
    "                augmented_text = aug5.augment(text)\n",
    "            else:\n",
    "                augmented_text = aug6.augment(text)\n",
    "            # elif n == 7:\n",
    "            #     augmented_text = aug7.augment(text)\n",
    "            # else:\n",
    "            #     augmented_text = aug8.augment(text)\n",
    "            # m = random.choice(sim)\n",
    "            # if m == 1:\n",
    "            #     augmented_we = aug2.augment(text)\n",
    "            # else:\n",
    "            #     augmented_we = aug0.augment(text)\n",
    "        except:\n",
    "            print('error is in line:', no)\n",
    "        w.writelines(augmented_text)\n",
    "        w.writelines('\\n')\n",
    "        # w.writelines(augmented_we)\n",
    "        # w.writelines('\\n')\n",
    "        # \n",
    "    print('ok!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## AUGMENTATION LIBRRY NLPAUG\n",
    "# TODO: https://github.com/makcedward/nlpaug#installation\n",
    "# "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"MODEL_DIR\"] = '../model'\n",
    "import nlpaug.augmenter.char as nac\n",
    "import torch\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "from nlpaug.util import Action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Augmented Text:\n",
      "airlibes Sales Representative\n",
      "Augmented Text:\n",
      "AicrlineGs Sales Representative\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# text = 'Jeffrey Halim'\n",
    "\n",
    "# Substitute character by keyboard distance\n",
    "aug = nac.KeyboardAug()\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)\n",
    "\n",
    "# Insert character randomly\n",
    "aug = nac.RandomCharAug(action=\"insert\")\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9d36fd1483d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## word augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Substitute word by spelling mistake words dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpellingAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MODEL_DIR\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'spelling_en.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maugmented_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(\"Original:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/augmenter/word/spelling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dict_path, name, aug_min, aug_max, aug_p, stopwords, tokenizer, reverse_tokenizer, include_reverse, stopwords_regex, verbose)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_reverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minclude_reverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mskip_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_idxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/augmenter/word/spelling.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(self, force_reload)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_spelling_error_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_reverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/augmenter/word/spelling.py\u001b[0m in \u001b[0;36minit_spelling_error_model\u001b[0;34m(dict_path, include_reverse, force_reload)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSPELLING_ERROR_MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mspelling_error_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmwd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpelling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_reverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mSPELLING_ERROR_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspelling_error_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/model/word_dict/spelling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dict_path, include_reverse, cache)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_reverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minclude_reverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/model/word_dict/spelling.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/model/word_dict/spelling.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../modelspelling_en.txt'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../modelspelling_en.txt'",
     "output_type": "error"
    }
   ],
   "source": [
    "## word augmentation\n",
    "# Substitute word by spelling mistake words dictionary\n",
    "aug = naw.SpellingAug(os.environ[\"MODEL_DIR\"] + 'spelling_en.txt')\n",
    "augmented_texts = aug.augment(text, n=3)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "print(\"Augmented Texts:\")\n",
    "print(augmented_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "start!\n",
      "Augmented Text:\n",
      "angina Alexdander\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "text= \"Alexdander\"\n",
    "#Insert word randomly by word embeddings similarity\n",
    "print('start!')\n",
    "# model_type: word2vec, glove or fasttext\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path='/Users/macos/Desktop/Edison-ai/models/GoogleNews-vectors-negative300.bin',\n",
    "    action=\"insert\", aug_max=2)\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "start!\n",
      "Augmented Text:\n",
      "Saturnin\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "text= \"Saturnin\"\n",
    "# text = \"Meissner Feusier\"\n",
    "#Insert word randomly by word embeddings similarity\n",
    "print('start!')\n",
    "# model_type: word2vec, glove or fasttext\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path='/Users/macos/Desktop/Edison-ai/models/GoogleNews-vectors-negative300.bin',\n",
    "    action=\"substitute\")\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "start!\n",
      "ok\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from nlpaug.util.file.download import DownloadUtil\n",
    "\n",
    "print('start!')\n",
    "DownloadUtil.download_glove(model_name='glove.6B', dest_dir='.')\n",
    "DownloadUtil.download_fasttext(model_name='wiki-news-300d-1M', dest_dir='.')\n",
    "print('ok')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "start\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bb23e93e3643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m aug = naw.WordEmbsAug(\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fasttext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MODEL_DIR\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     action=\"substitute\") #  aug_max = 2\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0maugmented_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner-tf/lib/python3.6/site-packages/nlpaug/augmenter/word/word_embs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_type, model_path, model, action, name, aug_min, aug_max, aug_p, top_k, n_gram_separator, stopwords, tokenizer, reverse_tokenizer, force_reload, stopwords_regex, verbose)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             self.model = self.get_model(model_path=model_path, model_type=model_type, force_reload=force_reload,\n\u001b[0;32m---> 87\u001b[0;31m                                         top_k=self.top_k)\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner-tf/lib/python3.6/site-packages/nlpaug/augmenter/word/word_embs.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(cls, model_path, model_type, force_reload, top_k)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_word_embs_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mskip_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_idxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner-tf/lib/python3.6/site-packages/nlpaug/augmenter/word/word_embs.py\u001b[0m in \u001b[0;36minit_word_embs_model\u001b[0;34m(model_path, model_type, force_reload, top_k)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fasttext'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFasttext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model type value is unexpected. Expected values include {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner-tf/lib/python3.6/site-packages/nlpaug/model/word_embs/fasttext.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, file_path, max_num_vector)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '../model'"
     ],
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '../model'",
     "output_type": "error"
    }
   ],
   "source": [
    "print('start')\n",
    "# text = \"Meissner Feusier\"\n",
    "# model_type: word2vec, glove or fasttext\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='fasttext', model_path= os.environ[\"MODEL_DIR\"],\n",
    "    action=\"substitute\") #  aug_max = 2\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "start!\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-694d266a1c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m aug = naw.ContextualWordEmbsAug(\n\u001b[0;32m----> 5\u001b[0;31m     model_path='bert-base-uncased', action=\"substitute\")\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0maugmented_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/augmenter/word/context_word_embs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, action, temperature, top_k, top_p, name, aug_min, aug_max, aug_p, stopwords, skip_unknown_word, device, force_reload, optimize, stopwords_regex, verbose)\u001b[0m\n\u001b[1;32m     91\u001b[0m         self.model = self.get_model(\n\u001b[1;32m     92\u001b[0m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             top_p=top_p, optimize=optimize)\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Override stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'xlnet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'roberta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/augmenter/word/context_word_embs.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(cls, model_path, device, force_reload, temperature, top_k, top_p, optimize)\u001b[0m\n\u001b[1;32m    272\u001b[0m     def get_model(cls, model_path, device='cuda', force_reload=False, temperature=1.0, top_k=None, top_p=0.0,\n\u001b[1;32m    273\u001b[0m                   optimize=None):\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_context_word_embs_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/augmenter/word/context_word_embs.py\u001b[0m in \u001b[0;36minit_context_word_embs_model\u001b[0;34m(model_path, device, force_reload, temperature, top_k, top_p, optimize)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRoberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m'bert'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m'xlnet'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXlNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/model/lang_models/bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, temperature, top_k, top_p, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# self.tokenizer = AutoTokenizer.from_pretrained(model_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# self.model = AutoModel.from_pretrained(model_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'BertTokenizer' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "# import torch\n",
    "# from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "print('start!')\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased', action=\"substitute\")\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/macos/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "ok\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/macos/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "ok\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "print('ok')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Original:\n",
      "Saturnin\n",
      "Augmented Text:\n",
      "Saturnin\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "text = \"Saturnin\"\n",
    "# Substitute word by WordNet's synonym\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-90b912f488b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# from pytorch_transformers import XLNetModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# model_path: xlnet-base-cased or gpt2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContextualWordEmbsForSentenceAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xlnet-base-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maugmented_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/augmenter/sentence/context_word_embs_sentence.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, temperature, top_k, top_p, name, device, force_reload, optimize, verbose)\u001b[0m\n\u001b[1;32m     75\u001b[0m         self.model = self.get_model(\n\u001b[1;32m     76\u001b[0m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             top_p=top_p, optimize=optimize)\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/augmenter/sentence/context_word_embs_sentence.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(cls, model_path, device, force_reload, temperature, top_k, top_p, optimize)\u001b[0m\n\u001b[1;32m    126\u001b[0m                   optimize=None):\n\u001b[1;32m    127\u001b[0m         return init_context_word_embs_sentence_model(model_path, device, force_reload, temperature, top_k, top_p,\n\u001b[0;32m--> 128\u001b[0;31m                                                      optimize=optimize)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/augmenter/sentence/context_word_embs_sentence.py\u001b[0m in \u001b[0;36minit_context_word_embs_sentence_model\u001b[0;34m(model_path, device, force_reload, temperature, top_k, top_p, optimize)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'xlnet'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         model = nml.XlNet(model_path, device=device, temperature=temperature, top_k=top_k, top_p=top_p,\n\u001b[0;32m---> 27\u001b[0;31m                           optimize=optimize)\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m'gpt2'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         model = nml.Gpt2(model_path, device=device, temperature=temperature, top_k=top_k, top_p=top_p,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nlpaug/model/lang_models/xlnet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, temperature, top_k, top_p, padding_text, optimize, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# TODO: Evaluted to use mems in XLNet but the result is quite weird.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'external_memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXLNetTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXLNetLMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'external_memory'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XLNetTokenizer' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'XLNetTokenizer' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "# from pytorch_transformers import XLNetModel\n",
    "# model_path: xlnet-base-cased or gpt2\n",
    "aug = nas.ContextualWordEmbsForSentenceAug(model_path='xlnet-base-cased')\n",
    "augmented_texts = aug.augment(text, n=3)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Texts:\")\n",
    "print(augmented_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "1.4.0\n",
      "2.2.1\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}